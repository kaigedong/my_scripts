为什么epoll (select/poll的升级版) 那么快？
有两个原因：

1、创建共有内存空间来维护文件句柄列表

我们自己写程序的轮询是程序维护一个文件句柄列表，保存文件句柄，进行轮询，让操作系统去查询文件是不是准备好了。
自己的程序在用户态(用户的内存空间中)，操作系统在内核态(内核的内存空间)，比如有4G的内存，操作系统占高1G内存地址，用户占低3G内存地址。

用户程序维护的文件句柄，需要拷贝到内核态来查询，内核态查询完之后，拷贝回给文件，这样来回非常占用资源。

而epoll是创建了一个独立的内存空间，这个内存空间既不属于内核，也不属于用户程序，而是**两者共有的**，这样，用户程序在请求系统查询的时候就不需要拷贝文件句柄了。

这是第一个改进的地方，相对于用户自己写的轮询的异步程序。

2、报告机制：不采用轮询

举个例子，在select/poll 中，当检测到文件句柄有变化时(有写入啥的)，需要轮询来看哪个文件句柄满足了可操作的条件，这个轮询是花费时间的。

而epoll 不是通过轮询，而是文件句柄自己报告，满足了条件。

poll最多只能有1024个文件句柄，这是跟内核啥的有关，想要改变它就需要重新编译内核；而select能通过**修改配置文件**的方式，改变文件句柄的最大数量 (因为文件句柄最多有多少个保存在一个文件里)。

比如我们有上万个文件连接，如果每个都很快接收到内容，那么采用select与epoll 效率差不多；而有很多个连接，只有其中几个接收到了内容，那么epoll就会快很多，这就是因为epoll的消息报告。

epoll 有三个关键函数，第一个是创建一个**红黑二叉树**，这是平衡二叉树里性能最好的一个，传入需要维护多少个文件句柄的int参数，然后创建内核和用户程序公用的空间来维护它，

然后返回一个int值：注意！这个返回值是红黑二叉树的头结点。

第二个关键函数是对红黑树进行的操作：添加节点、删除节点、修改节点：添加节点来对这个文件句柄(如socket)进行监控，或移出树，不再对其监控等

第三个是等待，在给定的time时间内，如果监控的文件句柄有事件发生，就返回用户态的进程。


到目前，还是不明白这个报告是怎么报告的，大概是第三个函数没理解秦楚。

这个内容似乎不错：https://blog.csdn.net/lixungogogo/article/details/52226479 虎头看看
确实不错，如下：

## epoll
在Linux网络编程中，很长时间都是使用select来做事件触发。在Linux新的内核中，有了一种替换它的机制，就是epoll。

相比于select，epoll最大的好处在于他不会随着监听fd数目的增长而降低效率。因为在内核中select的实现中，它是**采用轮询来处理的**，轮询的fd数目越多，耗时越多。

相对于select和poll来说，epoll更加灵活，没有描述符数量限制。

epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个`事件表`中，这样在用户空间和内核空间的copy只需要一次。

## epoll 接口
epoll操作过程需要3个接口，分别如下：
```
# include <sys/epoll.h>

int epoll_create(int size);
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
```
首先要调用epoll_create建立一个epoll对象。参数size是内核保证能够正确处理的最大文件句柄数，多于这个最大数时内核可不保证效果。

epoll_ctl 可以操作上面建立的epoll，例如，将刚建立的socket加入epoll中让其监控，或者把epoll正在监控的某个socket句柄移出epoll，不再监控它等等。

epoll_wait 在调用时，在给定timeout时间内，当监控的所有句柄中有事件发生时，就返回用户态的进程。

#### int epoll_create(int size)
创建一个epoll句柄，size用来告诉内核这个监听的数目一共有多大。这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值。

需要注意的是，当创建好epoll句柄后，它会占用一个fd值，在Linux下如果查看`/proc/进程ID/fd/`，是能够看到这个fd的，所以在使用完epoll后，必须调用`close()`关闭，否则可能导致fd被耗尽。

#### int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);

epoll的事件注册函数，它不同于select()是在监听事件时告诉内核要监听什么类型的事件，而是在这里先注册要监听的事件类型。

第一个参数是epoll_create()的返回值，第二个参数表示动作，第三个用宏来表示：
```
EPOLL_CTL_ADD: 注册新的fd到epfd中；
EPOLL_CTL_MOD: 修改已经注册的fd的监听事件；
EPOLL_CTL_DEL: 从epfd中删除一个fd；
```
第三个桉树是需要监听的fd
第四个参数告诉内核需要监听什么事，struct epoll_event结构如下：
```
struct epoll_event {
  __unit32_t events; /*Epoll events */
  epoll_data_t data; /*User data variable */
};
```

events 可以是一下几个宏的集合：
```
EPOLLIN: 表示对应的文件描述符可以读 (包括对端SOCKET正常关闭)；
EPOLLOUT: 表示对应的文件描述符可以写；
EPOLLPRI: 表示对应的文件描述符有紧急的数据可读 (这里应该表示有带外数据到来)；
EPOLLERR: 表示对应的文件描述符发生错误；
EPOLLHUP: 表示对应的文件描述符被挂断；
EPOLLET: 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的；
EPOLLONESHOT: 只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列中
```

#### int epoll_wait(int epfd, struct epoll_events *events, int maxevents, int timeout);
等待事件的产生，类似于select()调用

参数events用来从内核得到事件的集合，maxevents告诉内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size。

参数timeout是超时时间(毫秒，0会立即返回，-1将不确定，也有说法是永久阻塞)。该函数返回需要处理的事件数目，如果返回0表示已经超时。

epoll_wait返回之后应该是一个循环，遍历所有的事件。

我们调用epoll_wait 时就相当于以往调用select/poll，但是这时却不用传递socket给内核，因为内核已经在epoll_ctl中拿到了要监控的句柄列表。

所以，实际上在调用epoll_create后，内核就已经在内核态帮你存储要监控的句柄了，每次调用epoll_ctl只是在往内核的数据结构里塞入新的socket句柄。

在内核里，一切皆文件。所以，epoll向内核注册了一个文件系统，用于存储上述的被监控的socket。当你调用epoll_create时，就会在这个虚拟的epoll文件系统里创建一个file节点。当然这个file不是普通文件，它只是服务于epoll。

## epoll的实现机制
epoll在被内核初始化时(操作系统启动)，同时会开辟出epoll自己的内核高速cache区，用于安置每一个我们想监控的socket。

这些socket会以红黑树的形式保存在内核cache里，以支持快速的查找、插入、删除。

这个内核高速cache区，就是建立连续的物理内存页，然后在之上建立slab层。

简单的说，就是物理上分配好你想要的size的内存对象，每次使用都是使用空闲的已经分配好的对象。

epoll的高效就在于，当我们调用epoll_ctl往里塞入百万个句柄时，epoll_wait仍然可以飞快的返回，并有效的将发生事件的句柄给我们的用户。

这是由于我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建立了个file节点，在内核cache里建立了个红黑树用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件。

当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没有数据也返回。所以，epoll_wait非常高效。

而且，通常情况下即使我们要监控百万计的句柄，大多一次也只返回少量的准备就绪的句柄而已，所以，epoll_wait仅需要从内核态copy少量的句柄到用户态而已。

**那么这个准备就绪的链表是怎么维护的呢？**

当我们准备epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪的list链表里。

所以，当一个socket上有数据到了，内核在吧网卡上的数据copy到内核中后就来把socket插入到准备就绪的链表里了。

如此，一棵红黑树，一张准备就绪的句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。

执行epoll_create时，创建了红黑树和就绪链表，执行epoll_ctl时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断时间来临时，向准备就绪的链表中插入数据。执行epoll_wait时，立刻返回准备就绪链表里的数据即可。

## 工作模式
epoll对文件描述符的操作有两种模式：LT(level trigger) 和 ET (edge trigger)。LT模式是默认模式，LT模式与ET模式的区别如下：

### LT 模式：
当epoll_wait检测到描述符事件，并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事。

### ET模式
当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事。

ET模式在很大程度上减少了epoll事件被触发的次数，因此效率要比LT模式高。

epoll工作在ET模式的时候，必须使用非阻塞套接口，以免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。

那么ET模式是怎么做到的呢？

### ET模式的原理
当一个socket句柄上有事件时，内核会把该句柄插入上面所说的准备就绪list链表，这时我们调用epoll_wait，会把准备就绪的socket拷贝到用户态内存，然后清空准备就绪list链表。

最后，epoll_wait检查这些socket，如果不是ET模式(就是LT模式的句柄了)，并且这些socket上确实有未处理的事件时，又把该句柄放到刚刚清空的准备就绪链表了。

所以，非ET的句柄，只要它上面还有事件，epoll_wait每次都会返回。而ET模式的句柄，除非有新的中断到，即使socket上的事件没有处理完，也是不会次次从epoll_wait返回的。

### 优点

1）支持一个进程打开大数目的socket描述符(FD)

　　select最不能忍受的是一个进程所打开的FD是有一定限制的，由FD_SETSIZE设置，默认值是1024/2048。对于那些需要支持的上万连接数目的IM服务器来说显然太少了。这时候你一是可以选择修改这个宏然后重新编译内核。不过 epoll则没有这个限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。

2）IO效率不随FD数目增加而线性下降

　　传统的select/poll另一个致命弱点就是当你拥有一个很大的socket集合，不过由于网络延时，任一时间只有部分的socket是”活跃”的，但是select/poll每次调用都会线性扫描全部的集合，导致效率呈现线性下降。但是epoll不存在这个问题，它只会对”活跃”的socket进行操作—这是因为在内核实现中epoll是根据每个fd上面的callback函数实现的。那么，只有”活跃”的socket才会主动的去调用 callback函数，其他idle状态socket则不会，在这点上，epoll实现了一个”伪”AIO，因为这时候推动力在Linux内核。
3）使用mmap加速内核与用户空间的消息传递。

　　这点实际上涉及到epoll的具体实现了。无论是select,poll还是epoll都需要内核把FD消息通知给用户空间，如何避免不必要的内存拷贝就很重要，在这点上，epoll是通过内核与用户空间mmap同一块内存实现的。










